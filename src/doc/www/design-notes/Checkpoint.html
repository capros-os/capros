<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<html>
  <head>
    <title>The Checkpoint Mechanism</title>
    <meta name=linknotify content=all>
    <meta name=author content="Jonathan Shapiro">
    <link rel=author rev=made href="mailto:shap@eros-os.org"
	  title="Jonathan S. Shapiro">
  </head>
  <BODY BGCOLOR="#fff0ff" text="#000000" link="#0000ee" vlink="#551a8b" alink="#ff0000">
    <center>
      <h1 class=title>The Checkpoint Mechanism</h1>
    </center>
	  <p>
	    This note documents the design for the
	    checkpoint data structures and algorithms.
	  </p>
	  <h2>1. Background</h2>
	  <p>
	    The CapROS checkpoint design differs significantly from the
	    KeyKOS design, but many of the <em>ideas</em> of the
	    KeyKOS design still pertain.  The KeyKOS design is
	    described in <a
	      href="http://www.cis.upenn.edu/~KeyKOS/Checkpoint.html"><em>The 
		Checkpoint Mechanism in KeyKOS</em></a>.  The
	    differences between the two systems can be summarized as
	    follows:
	  </p>
	  <ul>
	    <li>
	      <p>
		KeyKOS uses two checkpoint areas of equal size (what
		Landau calls the <em>checkpoint area</em> and the
		<em>working area</em>).  It fills the working area up,
		declares a checkpoint, swaps the checkpoint and the
		working areas, and then drains (migrates) from what is
		now the checkpoint area.
	      </p>
	      <p>
		CapROS uses a single checkpoint area in a circular
		fashion, and divides this area dynamically into
		several generations.  A generation is permitted to
		occupy up to a fixed percentage of the checkpoint area
		(typically 65%) called the <em>limit-percent</em>
		before a checkpoint must be declared.
		A new generation is then started and a previous checkpoint
		generation is migrated.
		This space utilization strategy provides greater flexibility
		in the face of bursts of
		object modifications.  It also allows the checkpoint
		area to grow more easily.
	      </p>
	    </li>
	    <li>
	      <p>
		KeyKOS was designed for the checkpoint areas to be
		somewhat larger than main storage. It calculated its
		"Checkpoint Limit" based on the number of page frames,
		node frames, and directories, on the assumption that
		all of them could need to be written into the
		checkpoint. EROS and CapROS use a "reserve space
		before changing" technique which allows the checkpoint
		areas to be smaller than main storage.
	      </p>
	    </li>
	    <li>
	      <p>CapROS can have several generations of checkpoints 
		which have not yet been migrated (several active
		checkpoint areas).
		While KeyKOS had logic
		which skipped migration of a page (or node) which had been 
		changed since the checkpoint was taken (because it would be
		included in the next checkpoint),
		some of the frequently changed objects would be migrated
		before they were changed in the current generation. By
		not starting migration until more than one checkpoint
		has been taken, active pages and nodes have a chance
		to be modified before they are even considered for migration.
	      </p>
	      <p>While there is more than one checkpoint outstanding,
		this version of the checkpoint logic does not permit
		restart from other than the most recently stabilized
		checkpoint.
	      </p>
	    </li>
	    <li>
	      <p>In order to support Non-volatle RAM devices (NV RAM), which
		wear out after a limited number of write cycles,
		CapROS keeps a large number of checkpoint header frames
		at the beginning of the log area. This strategy limits
		the number of writes to a single header, and reduces
		the "heat" of this natural hot spot.
	      </p>
	      <p>If we assume 1 checkpoint/second and a design life of 
		ten years, then there will be about 315,576,000
		checkpoints taken. If we assume the device has
		a life-span
		of 1,000,000 writes before device failure, then 
		we will need 316 checkpoint header frames.
	      </p>
	    </li>
	    <li>
	      <p>
		KeyKOS never reuses space in the checkpoint area; if
		an object is modified a second time it gets a new
		location.  The reasons for this choice are to avoid
		seeks and head settling delays where possible,
		and to permit chained writes on the 370 disks which
		allowed actual data rates to approach the theoretical
		maximum.
	      </p>
	      <p>EROS adopted a strategy of re-writing a page in the
		original working area location if it was paged out twice
		during one checkpoint interval. This strategy had the
		advantage of reducing seek distances within a checkpoint.
	      <p>
		Because of the need to support NV RAM for checkpoints 
		CapROS will do the follow the KeyKOS strategy. This
		strategy has the advantage of eliminating a source of
		"hot spots" on the disk.
	      </p>
	    </li>
	    <li>
	      <p>
		Once KeyKOS begins to reuse a checkpoint area, all
		memory of that area's prior content is discarded.
	      </p>
	      <p>
		EROS preserves a record of object locations in the
		checkpoint area until their storage is actually
		reused. This strategy should reduce the number of
		seeks to areas outside the checkpoint area, improving
		disk performance. It will also increase the number of
		locations for a page or node, allowing fetches to be
		scheduled on less-active devices. 
	      </p>
	      <p>
		Which strategy CapROS adopts will be up to the person
		writing the code.
	      </p>
	    </li>
	    <li>
	      <p>
		EROS kept 28 bit log location IDs (<i>lid</i>, see
		below) in the Object Directory part of the checkpoint.
		Once you account for the 8 bits of node index,
		this size results in a maximum of
		2<sup>20</sup> frames in the log, which is barely enough to
		hold the RAM of a modern desktop processor. CapROS
		uses full-size <i>lid</i> entries in the Object Directory.
	      </p>
	    </li>
	    <li>
	      <p>
		EROS kept a 2 level tree for the on-disk Object
		Directory. This limited the size of a checkpoint to
		259,080 objects. CapROS chains Object Directory pages
		together, so there is no artifical limit on the number
		of objects in a checkpoint. CapROS uses the same
		technique for the Process Directory in the checkpoint.
	      </p>
	    </li>
	    <li>
	      <p>EROS required sequential numbering of the log
	      ranges. CapROS may relax that requirement depending on
	      how easy it is in practice.
	      </p>
	    </li>
	  </ul>
	  <h3>1.1 CapROS Definitions</h3>
	      <ul>
		<li><i>Active area</i> The area containing checkpoints
		  which have not been migrated.
		<li><i>Active checkpoint headers</i> The checkpoint
		  headers which describe the active area. They are the
		  checkpoint headers from most recently written
		  checkpoint header back to, but not including the
		  header pointed to by the hasMigrated field in the
		  most recently written checkpoint header.
		<li><i>Checkpoint Cursor</i> The next location to be
		  written in the log.
		<li><i>Checkpoint depth</i> The number of checkpoints
		  between the most recent checkpoint and the one being
		  migrated.
		<li><i>Limit Percent</i> The maximum amount of the
		space available for check points in the log which one
		checkpoint may use.
		<li><i>Stabilized checkpoint</i> A checkpoint where
		  all the pages, nodes, directories, etc. have been
		  written to disk. The last thing written in stabilizing
		a checkpoint is its checkpoint header.
		<li><i>Restart checkpoint header</i> The checkpoint
		  header of the most recent checkpoint during a restart.
		<li><i>Working area</i> The area being used for page
		  out operations as part of virtual memory
		  operations. It will contain the next checkpoint to be
		  declared.
	      </ul>
	  <h2>2. On-Disk Checkpoint Area</h2>
	  <p>
	    The CapROS checkpoint area is made up of ranges of disk page
	    frames that are sequentially numbered beginning at zero.
	    These are referred to as <b>log ranges</b>.  Just as page
	    frames in object ranges are referred to with <b>oid</b>s,
	    frames in the log are referred to with <b>lid</b>s.  A
	    <em>lid</em> is constructed by taking the 56 bit frame number
	    and concatenating an 8-bit object offset.
	    (We assume that 8 {bits) >=
	    log<sub>2</sub>(EROS_OBJECTS_PER_FRAME).)
	  </p>
	  <p>
	    Frames zero thru N hold the checkpoint headers of all the
	    stabilized checkpoints.  Any of these headers
	    may represent the most recently stabilized
	    checkpoint.  The most recently written header can be
	    determined by comparing the sequence numbers in the header
	    pages.  
	  </p>
	  <ul>
	      <pre>
 Frame 0       N-1                                             SIZE
+-------+~~~+--------+----------------------------------------+
|  hdr  |   |  hdr   | log page frames, dynamically allocated |
|   0   |   |  N-1   | to various checkpoint generations      |
+-------+~~~+--------+----------------------------------------+
 LID 0x0     (N-1)<<8 N<<8  ...                                SIZE<<8
	      </pre>
	  </ul>
	  <p>
	    Log ranges may reside on multiple disks, but the
	    implementation may require that sequential numbering be
	    preserved.  This requirement presents some difficulties
	    for checkpoint
	    area rearrangement which may, in due course, be
	    solved with suitable user-level applications.
	  </p>
	  <p>
	    A checkpoint that has been completely written to the disk
	    is said to be <b>stabilized</b>.  A stabilized checkpoint
	    has an associated on-disk directory that describes where
	    all of the objects in that checkpoint reside.  This
	    information is stored in a chained directory
	    structure. The last step in stabilizing a checkpoint is
	    writing the associated header.
	  </p>
	  <h3>2.1 The Checkpoint Header</h3>
	  <p>
	    The root of checkpoint directory is the checkpoint header.
	    The checkpoint header frame
	    begins with a header structure describing the checkpoint,
	    followed by the first (or only) part of the Process Directory.
	    <p>The last item in the
	    checkpoint header is a checksum of the previous
	    entries. This checksum is included to detect checkpoint
	    headers which were only partially written due to hardware
	    failure. We needed this field on the S/370 because
	    certain write failures (e.g. channel failure) caused the
	    disk controller to fill out the block with the last byte
	    transferred from the channel, and then write a
	    correct hardware checking block.
	  </p>
	  <ul>
	      <pre>
offset          contents
           +------------------+
0          |     Header       |
           +------------------+
hdrSize    | process dir lid  |
           |      ....        |
           | process dir lid  |
           +------------------+
pageSize-4 | checksum         |
           +------------------+
	      </pre>
	  </ul>
	  <p>
	    The header structure is:
	  </p>
	  <ul>
	      <pre>
struct DiskCheckpointHdr {
  uint64_t   sequenceNumber;
  uint64_t   hasMigrated;
  LID        firstLogLoc;
  LID        lastLogLoc;
  LID        firstDirPage;
  LID        firstProcessPage;
  uint32_t   nProcessPage;
};
	      </pre>
	  </ul>
          <p>Implementor note: The old 32 bit lid was referred to as
            a LogLoc in the code.</p>
	  <p>
	    Each of the fields is explained below.
	  </P>
	  <dl>
	    <dt><b>sequenceNumber</b></dt>
	    <dd>
	      <p>
		The sequence number allows CapROS to determine which
		checkpoint is most recent.  When restarting, CapROS
		first loads all the checkpoint headers from <tt>lid=0</tt>
		through <tt>lid=(n-1)&lt;&lt;8</tt>.
		If a drive failure occurred
		while writing a header (detected by bad CRC while
		reading the corresponding frame or by the checksum at
		the end of the header failing to verify), that header
		is discarded.  Of the valid headers, CapROS then chooses
		the header with the largest sequence number as the
		most recent checkpoint.
	      </p>
	      <p>
		Note that on recovery CapROS does not attempt to
		reconstruct <em>all</em> of the checkpoint generations 
		that may be present in the checkpoint area.  It loads
		the directories from the active checkpoint headers,
		and the process list from the most recent checkpoint header.
	      </p>
	    </dd>
	    <dt><b>hasMigrated</b></dt>
	    <dd>
	      <p>
		The ``has migrated'' field holds the sequence number
		of the latest checkpoint which was completely migrated
		when this checkpoint header was written. When
		restarting from a checkpoint, checkpoints
		taken after the "hasMigrated" checkpoint in the
		restart checkpoint header must be migrated, perhaps a
		second time.
	      </p>
	      <p>
		Once migration <em>has</em> completed, and another
		checkpoint has been taken, CapROS is free to
		begin reusing the object storage associated with a
		checkpoint area.  (Until a checkpoint header has been
		written indicating this checkpoint has been migrated,
		migration may restart when the system restarts.)
	      </p>
	      <p>
		The situation becomes more complex if the restart
		system is coded to allow restart from more than one
		checkpoint, or if it is desired to recover from a read
		error on a checkpoint header other than the most
		recent. The current design does not restart from other
		than the most recent checkpoint.
	      </p>
	    </dd>
	    <dt><b>firstLogLoc</b>, <b>lastLogLoc</b></dt>
	    <dd>
	      <p>
		The <tt>firstLogLoc</tt> and <tt>lastLogLoc</tt>
		fields allow the restart code to determine what part
		of the log contains this checkpoint. Note that
		<tt>firstLogLoc</tt> will be higher than
		<tt>lastLogLoc</tt> when this
		checkpoint wraps from the end of the log to the beginning.
		The restart code can use this span to check
		that all of the checkpoint area is mounted unless
		non-contigous log ranges are supported.
		A more alternate check whould be to check
		each <tt>lid</tt> when
		rebuilding the in-core object and process directories.
		If the most recent checkpoint is not all mounted, the
		system will not restart.
	      </p>
	    </dd>
	    <dt>
	      <b>firstDirPage</b>, <b>firstProcessPage</b>
	    </dt>
	    <dd>
	      <p>
		The <tt>firstDirPage</tt>
		and <tt>firstProcessPage</tt>
		gives the <em>lid</em> of the first page of the Object
		Directory and respectively the Process Directory.
		A zero entry indicates the end of the chain. Note that
		part of the process directory is held in the
		checkpoint header page. On an inactive system, that
		may the the entire process directory.
	      </p>
	    </dd>
	    <dt>
	      <b>nProcessPage</b>
	    </dt>
	    <dd>
	      <p>
		The <tt>nProcessPage</tt> gives the number of
		<em>lid</em> entries of Process Directory in the remainder of
		the checkpoint header.
	      </p>
	    </dd>
	  </dl>
	  <h3>2.2 The Process Directory</h3>
	  <p>
	    Each ``process directory lid'' in the checkpoint header
	    names the location of a process directory frame.  A process
	    directory frame consists of a <em>lid</em> for the next
	    process directory page, a single word describing the
	    number of following entries, and then some number of
	    process descriptors. With 8 byte OIDs and 4 byte ObCounts,
	    a 4K process directory frame can describe 340 processes.
	  </p>
	  <ul>
	      <pre>
struct processDirent {
  OID      oid;
  ObCount  allocCount;
};
	      </pre>
	  </ul>
	  <p>
	    An entry in the process directory corresponds to a process
	    that was running at the time the checkpoint was taken.
	    The process entries from the most current valid checkpoint
	    are reloaded as part of the system startup process.
	  </p>
	  <p>
	    The <tt>oid</tt> field identifies the process root node of
	    the this process.  The
	    <tt>allocCount</tt> field must match the allocation count
	    of the node.  The allocation count must be preserved
	    because processes can sleep for long periods of time during
	    which the nodes they occupy may be rescinded.
	  </p>
	  <h3>2.3 The Object Directory</h3>
	  <p>
	    The remaining directory lids in the checkpoint header
	    identify object directory frames.  Each entry holds an
	    object identifier, an allocation count, the type of the
	    object, and the <em>lid</em> at which the object can be
	    found. With 8 byte OIDs, 4 byte ObCounts, 8 byte LogLoc,
	    and 1 byte types, each 4k frame can hold a description of
	    163 objects (allowing for a count of entries and a chain
	    pointer at the start of the frame).
	  </p>
	  <ul>
	      <pre>
struct CkptDirent {
  OID       oid;
  ObCount   allocCount;
  ObCount   callCount
  LID       logLoc;
  uint8_t   type;
} ;
	      </pre>
	  </ul>
	  <p>
	    For pages and nodes, the allocCount field contains the
	    allocation count. For nodes, the callCount field contains
	    the call count. For pages the callCount field is zero.
	  </p>
	  <p>
	    In addition to a bit which tells if the OID is a page or a
	    node, the type field includes two bits which indicate
	    whether there are resume keys (callCountUsed) or other
	    keys (allocCountUsed) that may have the same value for the
	    respective count.
	  </p>
	  <p>
	    If the lid field of the directory is zero, then the
	    corresponding object is either a zero-filled page or a
	    node filled with null keys.
	    Which one can be determined by the value of the type field.
	  </p>
	  <p>
	    If the lid field is nonzero, the corresponding checkpoint
	    frame contains either page data or a
	    ``log pot.''  A log pot is simply a page-sized cluster of
	    nodes in the checkpoint area.  This is why the lid value
	    is concatenated with an object index: the index indicates
	    which entry in the log pot contains the relevant object.
	  </p>
	  <h2>3. Theory of Operation</h2>
          <p>
	    The basic idea behind the CapROS checkpoint design is that
	    we will divide the information in the checkpoint area into
	    several generations.  The <em>current</em> generation
	    corresponds very roughly to the paging area of conventional
	    operating systems, with the notable exception that
	    locations in it are not reused until the entire checkpoint
	    log has wraped. We define an instant in time as of which
	    the state of all processes and their pages and nodes are
	    saved as the time when the checkpoint
	    is <em>declared</em>.
	  </p>
          <p>
	    The <em>checkpoint</em> generation is
	    the most recently declared checkpoint, which may be only
	    partially written to the disk.  The <em>stable</em>
	    generations are the most recent <em>checkpoint depth</em>
	    checkpoints that have been entirely written to disk.
	  </p>
	  <center>
	    <img src="CheckpointLog.gif" alt="Checkpoint Log Arangement">
	  </center>
          <p>
	    At any given time, there may be a
	    checkpoint generation and there will be one or more stable
	    generations.  The
	    <em>migrated</em> generations remain in the checkpoint
	    area until their storage is reclaimed, but all the data in
	    them have been fully
	    copied to their home locations, and are shown as available
	    space in the above diagram.
	  </p>
	  <p>
	    Whenever an object in core is dirtied, space is reserved
	    for it in the current checkpoint generation. If
	    the total reserved space reaches the size of the available
	    space, the process is made to wait until migration can
	    free some space and the priority of migration is increased. 
	  <p>
	    Execution proceeds until the checkpoint interval
	    (typically 5 minutes) has passed or the current generation
	    has come to occupy more than some fixed percentage of the
	    checkpoint area (typically 65%).  When either event
	    occurs, a check is made to see if there is a previous
	    checkpoint generation which has not yet been
	    stabilized. If the previous generation has been
	    stabilized, a new checkpoint is declared.
	  </p>
	  <p>
	    Once the checkpoint is declared, the current generation
	    becomes the checkpoint generation, and a new current
	    generation is started.  Checkpointed objects in core are
	    marked "copy on write", and execution resumes.  All of
	    the in-core objects associated with the current
	    generation are then written out to the disk asynchronously
	    by the checkpointer process.
	  </p>
	  <p>
	    At any given moment, there is either a checkpoint
	    generation or a stable generation.  When the last
	    in-core object in the checkpoint generation has been
	    written to disk, along with the process directory and
	    object directory pages, a new checkpoint header is written and
	    that generation becomes a stable
	    generation.  The new checkpoint includes the sequence
	    number of the last generation that has been completely
	    migrated. That generation, and any older generations
	    become available for re-allocation. (Note that the
	    successfully migrated generations must be saved on disk so
	    migration will start with the correct generation after a
	    system restart.
	  </p>
	  <p>
	    If migration is not already running, the migrator will be
	    started on the oldest unmigrated generation. It will
	    attempt to migrate all subsiquent generations up to, but
	    not including the last stabilized generation.
	  </p>
	  <h2>4. Migration</h2>
	  <p>
	    Conceptually, during the migration phase, all the objects
	    in a stable
	    generation are copied from the stable generation to their
	    home locations on the disk.  Once this has completed, the
	    generation is considered to have migrated, and the
	    next-to-be-written on-disk
	    checkpoint header is written to
	    indicate that migration is done.
	  </p>
	  <p>
	    There is one important optimization which greatly reduces
	    the amount of migration that needs to be done. This
	    optimization is based on the observation that if an object
	    has changed since it was written into a stable generation,
	    then it does not need to be migrated because either it is
	    in a later stable generation, or it will be written as
	    part of the next checkpoint. If a restart occurs, it will
	    again be a candidate for migration, and the test for a
	    more recent version will be made then. While still useful,
	    this optimization becomes less useful when restart is
	    permitted from more than one checkpoint.
	  </p>
	  <p>
	    Migration is optimized by a non-persistant process
	    external to the kernel. This process may chose to optimize
	    disk arm motion in the home ranges if the log is
	    implemented on solid-state memory. The use of a
	    non-persistant process keeps the external migrator from
	    the danger of being stalled attempting to dirty an object.
	  </p>
          <p>
	    If an attempt is made to reserve the last available frame
	    in the log, then migration must be run to free up space,
	    before more objects may be changed. The dirting process
	    will be stalled, which may
	    impact its real-time performance.
          </p>
	  <h2>5. The Checkpoint Map</h2>
	  <p>
	    The checkpoint area storage is interpreted simultaneously 
	    at two layers:
	  </p>
	  <ul>
	    <li>
	      <p>
		As a set of page-sized frames, each of which is either
		<em>allocated</em>, <em>reserved</em>, or
		<em>free</em>.
	      </p>
	    </li>
	    <li>
	      <p>
		As a set of object containers, each of which is one
		page in size.  A given object container may hold a
		data page, a log pot of nodes, a process directorty
		entry, or a directory
		entry.
	      </p>
	      <p>
		If a frame is allocated, then there exists exactly one
		checkpoint generation associated with that frame.
	      </p>
	    </li>
	  </ul>
	  <h3>5.1 Frame Reservation</h3>
	  <p>
	    Before any object can be dirtied, space for it must be
	    reserved in the current checkpoint.  Herein lies the first
	    source of complexity.  Because there are multiple nodes
	    (directory entries) per frame, the checkpoint logic must
	    maintain reservation information at two levels for such
	    objects.  When allocating a new node (directory entry)
	    would overflow the current frame, a new frame must be
	    reserved. Space reservation must also account for object
	    directory, and process directory space.
	  </p>
	  <p>
	    The catch is that we do not actually decide where to store
	    the newly dirtied object when space for it is reserved.
	    Instead, we simply keep a count of how many objects of
	    each type have been reserved.  The reason for this is that
	    a long time may go by between dirtying the object and
	    writing it out to disk, and we would therefore like to
	    decide as late as possible the location in the log at
	    which the object will be written.
	  </p>
	  <h3>5.2 Frame Allocation</h3>
	  <p>
	    At some later time, we actually write the dirty object to
	    the disk.  At this point we actually allocate the log
	    frame that the object will go to.  If multiple objects
	    will fit in the frame, a count of the number of slots
	    allocated is kept in addition to the count of the number
	    of allocated frames.
	  </p>
	  <p>
	    Here we arrive at the second complication.  An object can
	    be dirtied (reserving space), written out (consuming
	    space), and subsequently <em>redirtied</em>.  When this
	    occurs, there are two possible design options:
	  </p>
	  <ul>
	    <li>
	      <p>
		Rewrite the object to its previously assigned
		location.
	      </p>
	    </li>
	    <li>
	      <p>
		Assign a new location to the object.
	      </p>
	    </li>
	  </ul>
	  <p>
	    To avoid "hot spots" on NV RAM, we adopt the second
	    policy. Even if a frame in the log does not contain an
	    active entry, we do not reuse it. As a result of these
	    decisions, we use a simple counter to allocate space in
	    the log.
	  </p>
	  <p>
	    A great many of the objects written to the checkpoint area will
	    prove to be zero objects.  For our purposes, a zero object
	    is any of the following:
	  </p>
	  <ul>
	    <li>
	      <p>
		A zero-filled data page.
	      </p>
	    </li>
	    <li>
	      <p>
		A node filled with zero number capabilities.
	      </p>
	    </li>
	  </ul>
	  <p>
	    The reason these occur with such frequency is that
	    returning storage to the space bank causes that storage to
	    be zeroed.  <em>This is not currently true, but we should
	    make it true.  The reason to do it is that objects
	    returned to the space bank are generally dirty, and
	    handling them this way obviates the need to rewrite
	    now-dead data back to the checkpoint area (or, in the case
	    of pages, to the home locations).  By storing them as zero
	    objects, the write bandwidth requirements can be reduced
	    significantly.</em>
	  </p>
	  <p>
	    The advantage to handling zero objects specially is that
	    they do not occupy any object storage in the checkpoint log.
	    The directory entry for a zero object suffices to indicate
	    that the object is zeroed.
	  </p>
	  <h3><a name="cssize">5.3 Algorithms</a></h3>
	  <p>
	    There are two algorithms of interest in the checkpoint
	    logic: reserving the storage for an object and writing an
	    object to the disk.
	  </p>
	  <h4>5.3.1 Space Reservation</h4>
	  <p>
	    Just before an object is dirtied, the storage reservation
	    algorithm is executed. (Note that the object is clean at
	    the time the checkpoint is declared, so its space is
	    reserved in the checkpoint that will be current after a
	    checkpoint is declared. The algorithm goes like this:
	  </p>
	  <ul>
	      <pre>
If space available is less than maximum available times <em>Limit Percent</em>
  AND the last declared checkpoint is stabilized
    reduce space available by the amount reserved
    set reserved to zero (we're starting a new checkpoint)
    declare checkpoint
If no directory space for the object has been reserved
    reserve in-core directory space for object
    reserve on-disk directory space for object
Reserve space for non-zero object
	      </pre>
	      <p>
		Note that if any of these reservations result in space
		available going to zero, all the reservations made
		for this call are released and the process is made to
		wait for migration to free more space.
	      </p>
	  </ul>
          <p>
	    Defining 1st active frame as the first frame in the active
	    area, we get:
	  </p>
	  <pre>
    Maximum available (MA) = log size - number of checkpoint headers
    Space used = (checkpoint cursor - 1st active frame + MA) mod MA
    Space available = MA - space used.
	  </pre>
	  <p>
	    Note the check for checkpoint size limits.
	    This is where a checkpoint will be declared for reasons of
	    space exhaustion.  Until the time comes to actually write
	    the new object, we must assume that it will be non-zero
	    and allocate space accordingly.
	  </p>
	  <h5>5.3.1.1 Potential Implementation Trap</h5>
	  <p>
	    The deallocation of the on-disk object can, under one
	    boundary condition, cause minor trouble.
	  </p>
	  <p>
	    Nodes go to disk in page-sized chunks called <em>log
	    pots</em>.  Suppose that a node is dirtied, written to the
	    log pot that we are currently assembling (at which point
	    the node is cleaned), and subsequently redirtied.  When it 
	    is redirtied, the allocation count associated with the log 
	    pot is decremented.  If it chances that this decrement
	    causes the log pot allocation count to go to zero, the
	    pending log pot will be incorrectly considered
	    reclaimable.
	  </p>
	  <p>
	    This can only arise under conditions of total memory
	    starvation, and yes, we found this the hard way.  The
	    solution is to simply maintain an "extra" allocation on
	    the log pot while its construction is in progress.
	  </p>
	  <h4>5.3.2 Object Write</h4>
	  <p>
	    The other algorithm of major interest is the object write
	    algorithm.  The main source of interest is the handling of
	    objects that prove to be zero objects when the time comes
	    to write them.
	  </p>
	  <ul>
	      <pre>
If object is zero object (increase available space)
    unreserve disk space for object
    update in-core checkpoint directory entry accordingly
else
    write object to disk and update in-core directory for disk location
	      </pre>
	  </ul>
	  <h3>5.4 Restart Actions</h3>
	  <p>
	    When CapROS is restarted after a shutdown (orderly or
	    otherwise), it finds the most recent checkpoint
	    header by reading all the checkpoint headers and locating
	    the one with the highest sequenceNumber. This header is
	    the restart checkpoint header. The hasMigrated value in
	    that header is
	    used to determine the active area, and those checkpoint
	    headers are used to reload the object directory.
	    The process directory is reloaded from the restart checkpoint
	    header.
	  </p>
	  <p>
	    While reloading the object directory, the restart logic
	    determines the highest
	    and lowest <em>lids</em> in the active area, and whether
	    the active area wraps around the end of the log area.
	    The rest of
	    the log is available for allocation. Allocation starts
	    with the <em>lid</em> which is one greater than the hi-lid
	    of the active area, wraping to the first <em>lid</em>
	    after the checkpoint headers. 
	  </p>
	  <p>
	    The original KeyKOS design did not reclaim checkpoint
	    frames, and therefore used a simple counter rather than a
	    counted map.  The EROS developers observed that log pots
	    containing ``hot'' nodes frequently empty themselves after
	    being paged out, and it is beneficial to be able to reuse
	    them. Since CapROS is designed to work with NV RAM,
	    CapROS uses the KeyKOS counter technique for log
	    allocation to avoid ``hot'' spots.
	  </p>
	  <h2>6. The Core Directory</h2>
	  <p>
	    There is a main core directory for all the stable
	    generations in the log. If there is no
	    checkpoint generation, i.e. the last checkpoint has
	    stabilized, the main directory will also hold entries
	    for the current area. If there is a checkpoint generation,
	    the main directory hold the entries for it and a "current"
	    directory is created for the current generation. (The
	    current directory
	    must be searched before the main directory when looking
	    for the current
	    version of an object). When a checkpoint generation
	    stabilizes, the current directory is merged back into the
	    main directory. 
	  </p>
	  <p>
	    Note that the migrator will have to be able to find
	    entries for the generation being migrated, so a compressed
	    generation number will need to be saved for each directory
	    entry or they can be linked in a list headed in the
	    core generation structure.
	  </p>
	  <ul>
	      <pre>
struct CoreDirectory {
  uint64_t migratorSequenceNumber;
  CoreDirent *oidTree;

};
	      </pre>
	  </ul>
	  <dl>
	    <dt><b>migratorSequenceNumber</b></dt>
	    <dd>
	      <p>
		The last completely migrated checkpoint sequence number.
	      </p>
	    </dd>
	    <dt><b>oidTree</b></dt>
	    <dd>
	      <p>
		The top of the red-black tree that makes up the
		in-core directory.
	      </p>
	    </dd>
	  </dl>
	  <h2>7. The Core Generation Structure</h2>
          <p>
	    <em>I (wsf) think this structure is overkill, but it is
	    useful documentation of what the coder will possibily
	    need. This section should be updated when the code is
	    written.</em>
	  </p>
	  <p>
	    Each checkpoint generation has an associated in-core
	    structure.  This structure maintains book-keeping
	    information concerning the checkpoint and contains the
	    root pointer for the in-core checkpoint directory.
	  </p>
	  <ul>
	      <pre>
struct CoreGeneration {
  bool canReclaim;
  CoreDirent *oidTree;
  uint32_t nCoreDirent;
  uint32_t nDirent;
  uint32_t nReservedFrames;
  uint32_t nAllocatedFrames;

  uint32_t nDirPage;
  uint32_t nLogPot;

  uint32_t nZeroPage;
  uint32_t nZeroNode;
  uint32_t nPage;
  uint32_t nNode;
  uint32_t nProcess;

  // Record of storage we have released:
  uint32_t nReleasedNode;
  
#ifdef PARANOID_CKPT
  uint32_t nAllocDirPage;
  uint32_t nAllocPage;
  uint32_t nAllocLogPot;
#endif
  
  LogLoc curLogPot;
  uint32_t   nNodesInLogPot;
};
	      </pre>
	  </ul>
	  <p>
	    For each type of object (page, zero page, node, zero node,
	    process), the core generation structure records
	    how many of these objects have been allocated in this
	    generation.  An object is allocated the first time it is
	    written to the checkpoint area.  Nodes are 
	  </p>
	  <p>
	    The various fields are:
	  </p>
	  <dl>
	    <dt><b>canReclaim</b></dt>
	    <dd>
	      <p>
		Indicates whether the objects in this checkpoint
		generation have been migrated, and can therefore be
		reclaimed.
	      </p>
	    </dd>
	    <dt><b>oidTree</b></dt>
	    <dd>
	      <p>
		Root pointer to a red-black tree of directory entries
		for each object in this generation.
	      </p>
	    </dd>
	    <dt><b>nCoreDirent</b></dt>
	    <dd>
	      <p>
		Number of entries in the core directory tree for this
		generation.
	      </p>
	    </dd>
	    <dt><b>nDirent</b></dt>
	    <dd>
	      <p>
		Number of on-disk directory entries that have been
		reserved for this generation.  Should always be equal
		to <tt>nCoreDirent</tt>.
	      </p>
	    </dd>
	    <dt><b>nReservedFrames</b></dt>
	    <dd>
	      <p>
		Number of disk frames that have been reserved for this 
		generation.
	      </p>
	    </dd>
	    <dt><b>nAllocatedFrames</b></dt>
	    <dd>
	      <p>
		Number of disk frames that have been <em>allocated</em> for this 
		generation.  Should always be less than or equal to
		<tt>nReservedFrames</tt>.
	      </p>
	    </dd>
	    <dt><b>nDirPage</b>,
	      <b>nPage</b>,
	      <b>nLogPot</b>,
	    </dt>
	    <dd>
	      <p>
		Number of directory pages, pages, and log pots
		(respectively) that have been reserved in this
		generation.  These should sum to <tt>nReservedFrames</tt>.
	      </p>
	    </dd>
	  </dl>
	  <ul>
	      <pre>
struct CoreDirent {
  CoreDirent *left;
  CoreDirent *right;
  CoreDirent *parent;

  OID         oid;
  ObCount     count;
  LogLoc      logLoc : 24;
  uint8_t        type;


  enum { red = 1, black = 0 };
  
  uint8_t        color;
};
	      </pre>
	  </ul>
  <h2>Appendix A - Another approach to NV RAM support</h2>
    <p>
      The design selected for CapROS above tries to avoid "hot spots"
      on disk, but still has "warm spots" in the checkpoint headers
      and possibly in the home locations. By having a large number of
      checkpoint headers, the heat in that part of the NV RAM is
      reduced to a level which should meet the device life requirements.
    </p>
    <p>
      The following design is based on the goal of never re-writing a
      location in the log until all other locations have been written
      (exactly once). It was rejected because:
    </p>
    <ol>
      <li>It uses cryptography. Using cryptography introduces the need
	for a good source of random numbers, and builds a dependence
	on the quality of cryptographic algorithms which is not
	present in CapROS for any other reason.
      </li>
      <li>It is vulnerable to any kind write error occuring when, after
	the log has wraped back to the beginning, the first checkpoint
	header is rewritten. An error rewriting the first checkpoint
	header will prevent restart from any checkpoint (because you
	won't be able to find them). This issue could be addressed
	by replicating the first checkpoint header (in lid=0 and
	lid=1).
      </li>
    </ol>
    <h3>The design</h3>
    <p>
      The log range page with LogDI (lid) 0 is the first checkpoint
      header. When the disk is formated, it is initialized to all zeroes.
      LogID 0 is always a checkpoint header, and acts as the root of all
      the checkpoint headers on the disk. All the log ranges on the
      system make up a single logical checkpoint area which is written
      sequentially in order of increasing LogIDs. When a checkpoint
      reaches the end of the available log ranges, it wraps around to the
      beginning, starting with lid=1.
    </p>
    <p>
      Each checkpoint header holds pointers to information about the
      checkpoint, more-or-less following those described in in the
      chosen design. In addition they hold forward and back pointers
      to checkpoint headers for previous and subsiquent checkpoints
      as follows:
    </p>
    <pre>
  HMACValue checksum;
  LID next;
  LID previous;
  LID hasMigrated;
  HMACKey nextMac;
  HMACKey previousMac;
  64bitint serialNumber;
    </pre>
    <p>
      The last operation in a checkpoint is writing the checkpoint header
      which is usually the first block of the checkpoint. (If the
      checkpoint wraps the log ranges, lid=0 will be the header for that
      checkpoint, and the first block of the checkpoint will either
      contain checkpoint data or remain unused.) Note that while we can
      calculate the lid of the next checkpoint, and generate a HMACKey
      for it, the header for that checkpoint will probably not contain a
      checkpoint header. It will probably be the contents of some old
      page, a log pot of nodes or checkpoint directory data. We use the
      HMAC to separate a checkpoint header from the other cases.
    </p>
    <p>
      The restart strategy is:
    </p>
    <pre>
  Read lid=0
  If that frame is all zeroes, there are no checkpoints. Stop.
  Do
    Read the next checkpoint header.
    If the HMAC does not check or the serialNumber is lower than
            the previous serialNumber break;
  Enddo
    </pre>
    <p>
      We how have the most recent checkpoint, and can restart from it.
      We can use the back pointers to find the checkpoint(s) that need
      migration. 
    </p>
    <h3>Additional Thought</h3>
    <p>
      We don't actually need "home ranges" at all. KeyKOS had the concept
      of "limbo", where, if when it came time to migrate a page or
      node, its home
      range wasn't mounted, the kernel simply marked it "dirty" and let
      it be written into the next checkpoint. This kind of logic could be
      used for all pages and nodes in the system, eliminating the home
      locations as hot spots.
    </p>
    <hr>
<table>
<tr valign=top>
  <td width=92>
<a href="http://sourceforge.net"><img src="http://sourceforge.net/sflogo.php?group_id=132228&amp;type=1" width="88" height="31" border="0" alt="SourceForge.net Logo" /></a>
  </td>
  <td>
      <em>Copyright 1998 by Jonathan Shapiro.
Copyright 2008 by Strawberry Development Group.  All rights reserved.
      For terms of redistribution, see the <a
      href="../legal/license/GPL.html">GNU General Public License</a></em>
This material is based upon work supported by the US Defense Advanced
Research Projects Agency under Contract No. W31P4Q-07-C-0070.
Approved for public release, distribution unlimited.
   </td>
</tr>
</table>
  </body>
</html>
