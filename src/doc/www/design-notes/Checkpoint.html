<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<html>
  <head>
    <title>The Checkpoint Mechanism</title>
    <meta name=linknotify content=all>
    <meta name=author content="Jonathan Shapiro">
    <link rel=author rev=made href="mailto:shap@eros-os.org"
	  title="Jonathan S. Shapiro">
  </head>
  <BODY BGCOLOR="#fff0ff" text="#000000" link="#0000ee" vlink="#551a8b" alink="#ff0000">
    <center>
      <h1 class=title>The Checkpoint Mechanism</h1>
    </center>
	  <p>
	    This note documents the design for the
	    checkpoint data structures and algorithms.
	  </p>
	  <h2>1. Background</h2>
	  <p>
	    The CapROS checkpoint design differs significantly from the
	    KeyKOS design, but many of the <em>ideas</em> of the
	    KeyKOS design still pertain.  The KeyKOS design is
	    described in <a
	      href="http://www.cis.upenn.edu/~KeyKOS/Checkpoint.html"><em>The 
		Checkpoint Mechanism in KeyKOS</em></a>.  The
	    differences between the two systems can be summarized as
	    follows:
	  </p>
	  <ul>
	    <li>
	      <p>
		KeyKOS uses two checkpoint areas of equal size (what
		Landau calls the <em>checkpoint area</em> and the
		<em>working area</em>).  It fills the working area up,
		declares a checkpoint, swaps the checkpoint and the
		working areas, and then drains (migrates) from what is
		now the checkpoint area.
	      </p>
	      <p>
		CapROS uses a single checkpoint area in a circular
		fashion, and divides this area dynamically into
		several generations.  A generation is permitted to
		occupy up to a fixed percentage of the checkpoint area
		(typically <b>??</b>%) before a checkpoint must be declared.
		A new generation is then started and a previous checkpoint
		generation is migrated.
		This space utilization strategy provides greater flexibility
		in the face of bursts of
		object modifications.  It also allows the checkpoint
		area to grow more easily.
	      </p>
	    </li>
	    <li>
	      <p>CapROS can have several generations of checkpoints 
		which have not yet been migrated (several active
		checkpoint areas).
		While KeyKOS had logic
		which skipped migration of a page (or node) which had been 
		changed since the checkpoint was taken (because it would be
		included in the next checkpoint),
		some of the frequently changed objects would be migrated
		before they were changed in the current generation. By
		not starting migration until more than one checkpoint
		has been taken, active pages and nodes have a chance
		to be modified before they are even considered for migration.
	      </p>
	    </li>
	    <li>
	      <p>In order to support Non-volatle RAM devices (NV RAM), which
		wear out after a limited number of write cycles,
		CapROS keeps a large number of checkpoint header frames
		at the beginning of the log area. This strategy limits
		the number of writes to a single header, and reduces
		the "heat" of this natural hot spot.
	      </p>
	      <p>If we assume 1 checkpoint/second and a design life of 
		ten years, then there will be about 315,576,000
		checkpoints taken. If we assume the device has
		a life-span
		of 1,000,000 writes before device failure, then 
		we will need 316 checkpoint header frames.
	      </p>
	    </li>
	    <li>
	      <p>
		KeyKOS never reuses space in the checkpoint area; if
		an object is modified a second time it gets a new
		location.  The reasons for this choice are to avoid
		seeks and head settling delays where possible,
		and to permit chained writes on the 370 disks which
		allowed actual data rates to approach the theoritical
		maximum.
	      </p>
	      <p>EROS adopted a strategy of re-writing a page in the
	      original working area location if it was paged out twice
	      during one checkpoint interval. This strategy had the
	      advantage of reducing seek distances within a checkpoint.
	      <p>
		Because of the need to support NV RAM for checkpoints 
		CapROS will do the follow the KeyKOS strategy. This
		strategy has the advantage of eliminating a source of
		"hot spots" on the disk.
	      </p>
	    </li>
	    <li>
	      <p>
		Once KeyKOS begins to reuse a checkpoint area, all
		memory of that area's prior content is discarded.
	      </p>
	      <p>
		EROS preserves a record of object locations in the
		checkpoint area until their storage is actually
		reused. This strategy should reduce the number of
		seeks to areas outside the checkpoint area, improving
		disk performance. It will also increase the number of
		locations for a page or node, allowing fetches to be
		scheduled on less-active devices. 
	      </p>
	      <p>
		Which strategy CapROS adopts will be up to the person
		writing the code.
	      </p>
	    </li>
	    <li>
	      <p>
		EROS kept 28 bit log location IDs (<i>lid</i>, see
		below) in the Object Directory part of the checkpoint.
		This size resulted in a maximum of
		2<sup>20</sup> frames in the log, which is barely enough to
		hold the RAM of a modern desktop processor. CapROS may
		use more. <i>Why does DiskCkpt.h claim 28 bits allows
		addressing 1,099,511,627,776 "pages" of data in the
		log? By my reconing, you lose 8 bits for the node
		index, leaving 20 bits for the frame index, which
		sounds like a 1,048,576 pages. - WSF</i> CapROS may
		change this value.
	      </p>
	    </li>
	  </ul>
	  <h3>1.1 CapROS Definitions</h3>
	      <ul>
		<li><i>Active area</i> The area containing checkpoints
		  which have not been migrated.
		<li><i>Active checkpoint headers</i> The checkpoint
		headers which describe the active area.
		<li><i>Stabilized checkpoint</i> A checkpoint where
		all the pages, nodes, directories, etc. have been
		written to disk. The last thing written in stabilizing
		a checkpoint is its checkpoint header.
		<li><i>Restart checkpoint header</i> The checkpoint
		header of the most recent checkpoint during a restart.
		<li><i>Working area</i> The area being used for page
		out operations as part of virtual memory
		operations. It will contain the next checkpoint to be
		declared.
	      </ul>
	  <h2>2. On-Disk Checkpoint Area</h2>
	  <p>
	    The CapROS checkpoint area is made up of ranges of disk page
	    frames that are sequentially numbered beginning at zero.
	    These are referred to as <b>log ranges</b>.  Just as page
	    frames in object ranges are referred to with <b>oid</b>s,
	    frames in the log are referred to with <b>lid</b>s.  A
	    <em>lid</em> is constructed by taking the page frame number
	    and concatenating an 8-bit object offset.
	    (We assume that 8 {bits) >=
	    log<sub>2</sub>(EROS_OBJECTS_PER_FRAME).)

	  </p>
	  <p>
	    Frames zero thru N hold the checkpoint headers of all the
	    stabilized checkpoints.  Any of these headers
	    may represent the most recently stabilized
	    checkpoint.  The most recently written header can be
	    determined by comparing the sequence numbers in the header
	    pages.  
	  </p>
	  <ul>
	      <pre>
 Frame 0       N-1                                             SIZE
+-------+~~~+--------+----------------------------------------+
|  hdr  |   |  hdr   | log page frames, dynamically allocated |
|   0   |   |  N-1   | to various checkpoint generations      |
+-------+~~~+--------+----------------------------------------+
 LID 0x0     (N-1)<<8 N<<8  ...                                SIZE<<8
	      </pre>
	  </ul>
	  <p>
	    Log ranges may reside on multiple disks, <i>but the present
	    implementation requires that sequential numbering be
	    preserved</i>???.  This presents some difficulties for checkpoint
	    area rearrangement that we will need in due course to
	    solve with suitable user-level applications.
	  </p>
	  <p>
	    A checkpoint that has been completely written to the disk
	    is said to be <b>stabilized</b>.  A stabilized checkpoint
	    has an associated on-disk directory that describes where
	    all of the objects in that checkpoint reside.  This
	    information is stored in a two-level directory
	    structure. The last step in stabilizing a checkpoint is
	    writing the associated header.
	  </p>
	  <h3>2.1 The Checkpoint Header</h3>
	  <p>
	    The root of checkpoint directory is the checkpoint header.
	    The checkpoint header frame
	    begins with a header structure describing the checkpoint,
	    followed by the <em>lid</em> values of each second-level
	    header in the checkpoint area.  
	    <br>
	    <i>Do we need? The last item in the
	    checkpoint header is a checksum of the previous
	    entries. This checksum is included to detect checkpoint
	    headers which were only partially written due to hardware
	    failure. We needed this field on the S/370 because
	    certain write failures (e.g. channel failure) caused
	    correct checking blocks to be written with made-up
	    data.</i> 
	  </p>
	  <p>
	    The checkpoint header has enough room for 1017
	    directory <em>lids</em>. If we assume there are fewer than
	    341 processes, so only one frame is needed to describe
	    them, then the checkpoint header and associated object
	    directory entries can describe a maximum of
	    259,080 pages or nodes, or slightly over 1 gigabyte
	    assuming 4K pages and very few nodes.
	  </p>
	  <ul>
	      <pre>
+-----------------+
|     Header      |
+-----------------+
| thread dir lid  |
|      ....       |
| thread dir lid  |
+-----------------+
| object dir lid  |
|      ....       |
| object dir lid  |
+-----------------+
	      </pre>
	  </ul>
	  <p>
	    The header structure is:
	  </p>
	  <ul>
	      <pre>
struct DiskCheckpointHdr {
  uint64_t   sequenceNumber;
  uint64_t   hasMigrated;
  LogLoc     maxLogLoc;

  uint32_t   nDirPage;
  uint32_t   nThreadPage;
};
	      </pre>
	  </ul>
	  <p>
	    Each of the fields is explained below.
	  </P>
	  <dl>
	    <dt><b>sequenceNumber</b></dt>
	    <dd>
	      <p>
		The sequence number allows CapROS to determine which
		checkpoint is most recent.  When restarting, CapROS
		first loads all the checkpoint headers from <tt>lid=0</tt>
		and <tt>lid=(n-1)<<8</tt>.  If a drive failure occurred
		while writing a header (detected by bad CRC while
		rereading the corresponding frame), that header
		is discarded.  Of the valid headers, CapROS then chooses
		the header with the largest sequence number as the
		most recent checkpoint.
	      </p>
	      <p>
		Note that on recovery CapROS does not attempt to
		reconstruct <em>all</em> of the checkpoint generations 
		that may be present in the checkpoint area.  It loads
		the directories from the active checkpoint headers,
		and the thread list from the most recent checkpoint header.
	      </p>
	    </dd>
	    <dt><b>hasMigrated</b></dt>
	    <dd>
	      <p>
		The ``has migrated'' field holds the sequence number
		of the latest checkpoint which was completely migrated
		when this checkpoint header was written. When
		restarting from a checkpoint, checkpoints
		taken after the "hasMigrated" checkpoint in the
		restart checkpoint header must be migrated, perhaps a
		second time.
	      </p>
	      <p>
		Once migration <em>has</em> completed, and another
		checkpoint has bee taken, CapROS is free to
		begin reusing the object storage associated with a
		checkpoint area.  (Until a checkpoint header has been
		written indicating this checkpoint has been migrated,
		migration may restart when the system restarts.)
	      </p>
	      <p>
		The situation becomes more complex if the restart
		system is coded to allow restart from more than one
		checkpoint, or if it is desired to recover from a read
		error on a checkpoint header other than the most
		recent. The current design does not restart from other
		than the most recent checkpoint.
	      </p>
	    </dd>
	    <dt><b>maxLogLoc</b></dt>
	    <dd>
	      <p>
		The <tt>maxLogLoc</tt> field allows the restart code
		to verify that it has recovered all of the checkpoint
		area.  <tt>maxLogLoc</tt> contains the number of log
		frames that were known to the system at the time of
		the last checkpoint.  If the restart process has not
		managed to build a complete checkpoint area of at
		least this number of frames, the system will not
		restart. <i>Do we need this field?</i>
	      </p>
	    </dd>
	    <dt>
	      <b>nThreadPage</b>,
	      <b>nDirPage</b>
	    </dt>
	    <dd>
	      <p>
		The <tt>nThreadPage</tt> and
		<tt>nDirPage</tt> give (respectively) the number of
		<em>lid</em> entries of each type in the remainder of
		the checkpoint header.
	      </p>
	    </dd>
	  </dl>
	  <h3>2.2 The Thread Directory <i>should be called Process
	      Directory?</i></h3>
	  <p>
	    Each ``thread directory lid'' in the checkpoint header
	    names the location of a thread directory frame.  A thread
	    directory frame consists of a single word describing the
	    number of following entries, and then some number of
	    thread descriptors. With 8 byte OIDs and 4 byte ObCounts,
	    a 4K thread directory frame can describe 341 threads.
	  </p>
	  <ul>
	      <pre>
struct ThreadDirent {
  OID      oid;
  ObCount  allocCount;
};
	      </pre>
	  </ul>
	  <p>
	    An entry in the thread directory corresponds to a process
	    that was running at the time the checkpoint was taken.
	    The thread entries from the most current valid checkpoint
	    are reloaded as part of the system startup process.
	  </p>
	  <p>
	    The <tt>oid</tt> field identifies the process root node of
	    the process currently occupied by this thread.  The
	    <tt>allocCount</tt> field must match the allocation count
	    of the node.  The allocation count must be preserved
	    because threads can sleep for long periods of time during
	    which the process they occupy may be rescinded.
<i>Is this correct?</i>
	  </p>
	  <h3>2.3 The Object Directory</h3>
	  <p>
	    The remaining directory lids in the checkpoint header
	    identify object directory frames.  Each entry holds an
	    object identifier, an allocation count, the type of the
	    object, and the <em>lid</em> at which the object can be
	    found. With 8 byte OIDs, 4 byte ObCounts, 3.5 byte LogLoc,
	    and .5 byte types, each 4k frame can hold a description of
	    255 objects (allowing for a count of entries at the start
	    of the frame).
	  </p>
	  <ul>
	      <pre>
struct CkptDirent {
  OID       oid;
  ObCount   count;
  LogLoc    logLoc : 28;
  uint8_t   type : 4;
} ;
	      </pre>
	  </ul>
	  <p>
	    For pages, the count field contains the allocation count.
	    <i>For nodes, where are the allocation count and call count
            located?</i> The allocation count is stored in this
            "count" field, the allocation is stored with the node
            in the log pot.
	  </p>
	  <p>
	    If the lid field of the directory is zero, then the
	    corresponding object is either a zero-filled page or a
	    node filled with null keys.
	    Which one can be determined by the value of the type field.
	  </p>
	  <p>
	    If the lid field is nonzero, the corresponding checkpoint
	    frame contains either page data or a
	    ``log pot.''  A log pot is simply a page-sized cluster of
	    nodes in the checkpoint area.  This is why the lid value
	    is concatenated with an object index: the index indicates
	    which entry in the log pot contains the relevant object.
	  </p>
	  <h2>3. Theory of Operation</h2>
	  <p>
	    The basic idea behind the CapROS checkpoint design is that
	    we will divide the information in the checkpoint area into
	    several generations.  The <em>current</em> generation
	    corresponds roughly to the paging area of conventional
	    operating systems.  The <em>checkpoint</em> generation is
	    the most recently declared checkpoint, which may be only
	    partially written to the disk.  The <em>stable</em>
	    generation is the most recent checkpoint that is entirely
	    written to the disk.  At any given time, there is either a
	    checkpoint generation or a stable generation.  The
	    <em>migrated</em> generations remain in the checkpoint
	    area until their storage is reclaimed, but have been fully
	    copied to their home locations.
	  </p>
	  <center>
	    <img src="ckpt-states.gif" alt="Checkpoint Generations">
	  </center>
	  <p>
	    Whenever an object in memory is dirtied, space is reserved
	    for it in the current checkpoint generation.  If
	    necessary, space will be made available by throwing away
	    objects in the oldest generation to create available
	    storage.
	  <p>
	    Execution proceeds until the checkpoint interval
	    (typically 5 minutes) has passed or the current generation
	    has come to occupy more than some fixed percentage of the
	    checkpoint area (typically <b>??</b>%).  When either event
	    occurs, a checkpoint is declared.  All generations move
	    one step to the right, and the rightmost generation is
	    discarded.  Objects may have been written to the current
	    generation due to ageing, but at the time the checkpoint
	    is declared many of these objects will still be in memory.
	  </p>
	  <p>
	    Once the checkpoint is declared, the current generation
	    becomes the checkpoint generation, and a new current
	    generation is started.  Checkpointed objects in memory are
	    marked ``copy on write,'' and execution resumes.  All of
	    the in-memory objects associated with the current
	    generation are then written out to the disk asynchronously
	    by the checkpointer process.
	  <p>
	    At any given moment, there is either a checkpoint
	    generation or a stable generation.  When the last
	    in-memory object in the checkpoint generation has been
	    written to disk, that generation becomes the stable
	    generation.  Migration now begins.
	  </p>
	  <p>
	    During the migration phase, objects in the stable
	    generation are copied from the stable generation to their
	    home locations on the disk.  Once this has completed, the
	    generation is considered to have migrated, and the on-disk
	    checkpoint header for this generation is rewritten to
	    indicate that migration is done.
	  </p>
	  <p>
	    In principle, CapROS can be configured for any number of
	    generations greater than three.  In practice, the current
	    system is configured for exactly three.
	  </p>
	  <h2>4. The Checkpoint Map</h2>
	  <p>
	    The checkpoint area storage is interpreted simultaneously 
	    at two layers:
	  </p>
	  <ul>
	    <li>
	      <p>
		As a set of page-sized frames, each of which is either
		<em>allocated</em>, <em>reserved</em>, or
		<em>free</em>.
	      </p>
	    </li>
	    <li>
	      <p>
		As a set of object containers, each of which is one
		page in size.  A given object container may hold a
		data page, a log pot of nodes, or a directory
		entry.  [They can also hold thread
		entries, but these are handled as special cases by the 
		checkpoint logic.]
	      </p>
	      <p>
		If a frame is allocated, then there exists exactly one
		checkpoint generation associated with that frame.
	      </p>
	    </li>
	  </ul>
	  <h3>4.1 Frame Reservation</h3>
	  <p>
	    Before any object can be dirtied, space for it must be
	    reserved in the current checkpoint.  Herein lies the first
	    source of complexity.  Because there are multiple nodes
	    (directory entries) per frame, the checkpoint logic must
	    maintain reservation information at two levels for such
	    objects.  When allocating a new node (directory entry)
	    would overflow the current frame, a new frame must be
	    reserved.
	  </p>
	  <p>
	    The catch is that we do not actually decide where to store
	    the newly dirtied object when space for it is reserved.
	    Instead, we simply keep a count of how many objects of
	    each type have been reserved.  The reason for this is that
	    a long time may go by between dirtying the object and
	    writing it out to disk, and we would therefore like to
	    decide as late as possible the location in the log at
	    which the object will be written.
	  </p>
	  <h3>4.2 Frame Allocation</h3>
	  <p>
	    At some later time, we actually write the dirty object to
	    the disk.  At this point we actually allocate the log
	    frame that the object will go to.  If multiple objects
	    will fit in the frame, a count of the number of slots
	    allocated is kept in addition to the count of the number
	    of allocated frames.
	  </p>
	  <p>
	    Here we arrive at the second complication.  An object can
	    be dirtied (reserving space), written out (consuming
	    space), and subsequently <em>redirtied</em>.  When this
	    occurs, there are two possible design options:
	  </p>
	  <ul>
	    <li>
	      <p>
		Rewrite the object to its previously assigned
		location.
	      </p>
	    </li>
	    <li>
	      <p>
		Assign a new location to the object.
	      </p>
	    </li>
	  </ul>
	  <p>
	    To avoid "hot spots" on NV RAM, we adopt the second
	    policy. Even if a frame in the log does not contain an
	    active entry, we do not reuse it. As a result of these
	    decisions, we use a simple counter to allocate space in
	    the log.
	  </p>
	  <p>
	    A great many of the objects written to the checkpoint area will
	    prove to be zero objects.  For our purposes, a zero object
	    is any of the following:
	  </p>
	  <ul>
	    <li>
	      <p>
		A zero-filled data page.
	      </p>
	    </li>
	    <li>
	      <p>
		A node filled with zero number capabilities.
	      </p>
	    </li>
	  </ul>
	  <p>
	    The reason these occur with such frequency is that
	    returning storage to the space bank causes that storage to
	    be zeroed.  <em>This is not currently true, but we should
	    make it true.  The reason to do it is that objects
	    returned to the space bank are generally dirty, and
	    handling them this way obviates the need to rewrite
	    now-dead data back to the checkpoint area (or, in the case
	    of pages, to the home locations).  By storing them as zero
	    objects, the write bandwidth requirements can be reduced
	    significantly.</em>
	  </p>
	  <p>
	    The advantage to handling zero objects specially is that
	    they do not occupy any storage in the checkpoint log.  The 
	    directory entry for a zero object suffices to indicate
	    that the object is zeroed.
	  </p>
	  <h3><a name="cssize">4.3 Algorithms</a></h3>
	  <p>
	    There are two algorithms of interest in the checkpoint
	    logic: reserving the storage for an object and writing an
	    object to the disk.
	  </p>
	  <h4>4.3.1 Space Reservation</h4>
	  <p>
	    When an object is dirtied, the storage reservation
	    algorithm goes as follows:
	  </p>
	  <ul>
	      <pre>
If no directory space for the object has been reserved
    reserve in-core directory space for object
    reserve on-disk directory space for object
Reserve space for non-zero object
If (reserved frames - released frames) exceeds 65%
    unreserve everything
    declare checkpoint
	      </pre>
	  </ul>
	  <p>
	    Note the check in the middle for checkpoint size limits.
	    This is where a checkpoint will be declared for reasons of
	    space exhaustion.  Until the time comes to actually write
	    the new object, we must assume that it will be non-zero
	    and allocate space accordingly.
	  </p>
	  <p>
	    The deallocation of the on-disk object can, under one
	    boundary condition, cause minor trouble.
	  </p>
	  <p>
	    Nodes go to disk in page-sized chunks called <em>log
	    pots</em>.  Suppose that a node is dirtied, written to the
	    log pot that we are currently assembling (at which point
	    the node is cleaned), and subsequently redirtied.  When it 
	    is redirtied, the allocation count associated with the log 
	    pot is decremented.  If it chances that this decrement
	    causes the log pot allocation count to go to zero, the
	    pending log pot will be incorrectly considered
	    reclaimable.
	  </p>
	  <p>
	    This can only arise under conditions of total memory
	    starvation, and yes, we found this the hard way.  The
	    solution is to simply maintain an ``extra'' allocation on
	    the log pot while its construction is in progress.
	  </p>
	  <h4>4.3.2 Object Write</h4>
	  <p>
	    The other algorithm of major interest is the object write
	    algorithm.  The main source of interest is the handling of
	    objects that prove to be zero objects when the time comes
	    to write them.
	  </p>
	  <ul>
	      <pre>
If object is zero object
    unreserve disk space for object
    update in-core checkpoint directory entry accordingly
else
    write object to disk
	      </pre>
	  </ul>
	  <h3>4.4 Restart Actions</h3>
	  <p>
	    When CapROS is restarted after a shutdown (orderly or
	    otherwise), it first locates the most recent checkpoint
	    header.  It reloads the thread directory,
	    and then checks to see if the checkpoint has migrated.  If 
	    not, it also reloads the object directory.
	  </p>
	  <p>
	    While doing this, the restart logic builds an in-core map
	    of which frames in the checkpoint area are part of
	    the current checkpoint, and are therefore not available
	    for allocation.  This is called the <b>checkpoint map</b>.
	  </p>
	  <p>
	    The checkpoint map maintains a <em>count</em>, for each
	    frame in the checkpoint area, of the number of
	    objects currently stored in that frame.  If a frame has no
	    objects stored, it is considered reclaimable.  The
	    checkpoint headers and directory frames are each considered
	    to contain one object.
	  </p>
	  <p>
	    The original KeyKOS design did not reclaim checkpoint
	    frames, and therefore used a simple bitmap rather than a
	    counted map.  In EROS, we have observed that log pots
	    containing ``hot'' nodes frequently empty themselves after
	    being paged out, and it is beneficial to be able to reuse
	    them.
	  </p>
	  <h2>5. The Core Generation Structure</h2>
	  <p>
	    Each checkpoint generation has an associated in-core
	    structure.  This structure maintains book-keeping
	    information concerning the checkpoint and contains the
	    root pointer for the in-core checkpoint directory.
	  </p>
	  <ul>
	      <pre>
struct CoreGeneration {
  bool canReclaim;
  CoreDirent *oidTree;
  uint32_t nCoreDirent;
  uint32_t nDirent;
  uint32_t nReservedFrames;
  uint32_t nAllocatedFrames;

  uint32_t nDirPage;
  uint32_t nLogPot;

  uint32_t nZeroPage;
  uint32_t nZeroNode;
  uint32_t nPage;
  uint32_t nNode;
  uint32_t nThread;

  // Record of storage we have released:
  uint32_t nReleasedNode;
  
#ifdef PARANOID_CKPT
  uint32_t nAllocDirPage;
  uint32_t nAllocPage;
  uint32_t nAllocLogPot;
#endif
  
  LogLoc curLogPot;
  uint32_t   nNodesInLogPot;
};
	      </pre>
	  </ul>
	  <p>
	    For each type of object (page, zero page, node, zero node,
	    thread), the core generation structure records
	    how many of these objects have been allocated in this
	    generation.  An object is allocated the first time it is
	    written to the checkpoint area.  Nodes are 
	  </p>
	  <p>
	    The various fields are:
	  </p>
	  <dl>
	    <dt><b>canReclaim</b></dt>
	    <dd>
	      <p>
		Indicates whether the objects in this checkpoint
		generation have been migrated, and can therefore be
		reclaimed.
	      </p>
	    </dd>
	    <dt><b>oidTree</b></dt>
	    <dd>
	      <p>
		Root pointer to a red-black tree of directory entries
		for each object in this generation.
	      </p>
	    </dd>
	    <dt><b>nCoreDirent</b></dt>
	    <dd>
	      <p>
		Number of entries in the core directory tree for this
		generation.
	      </p>
	    </dd>
	    <dt><b>nCoreDirent</b></dt>
	    <dd>
	      <p>
		Number of entries in the core directory tree for this
		generation.
	      </p>
	    </dd>
	    <dt><b>nDirent</b></dt>
	    <dd>
	      <p>
		Number of on-disk directory entries that have been
		reserved for this generation.  Should always be equal
		to <tt>nCoreDirent</tt>.
	      </p>
	    </dd>
	    <dt><b>nReservedFrames</b></dt>
	    <dd>
	      <p>
		Number of disk frames that have been reserved for this 
		generation.
	      </p>
	    </dd>
	    <dt><b>nAllocatedFrames</b></dt>
	    <dd>
	      <p>
		Number of disk frames that have been <em>allocated</em> for this 
		generation.  Should always be less than or equal to
		<tt>nReservedFrames</tt>.
	      </p>
	    </dd>
	    <dt><b>nDirPage</b>,
	      <b>nPage</b>,
	      <b>nLogPot</b>,
	    </dt>
	    <dd>
	      <p>
		Number of directory pages, pages, and log pots
		(respectively) that have been reserved in this
		generation.  These should sum to <tt>nReservedFrames</tt>.
	      </p>
	    </dd>
	  </dl>
	  <ul>
	      <pre>
struct CoreDirent {
  CoreDirent *left;
  CoreDirent *right;
  CoreDirent *parent;

  OID         oid;
  ObCount     count;
  LogLoc      logLoc : 24;
  uint8_t        type;


  enum { red = 1, black = 0 };
  
  uint8_t        color;
};
	      </pre>
	  </ul>
	  <hr>
<table>
<tr valign=top>
  <td width=92>
<a href="http://sourceforge.net"><img src="http://sourceforge.net/sflogo.php?group_id=132228&amp;type=1" width="88" height="31" border="0" alt="SourceForge.net Logo" /></a>
  </td>
  <td>
      <em>Copyright 1998 by Jonathan Shapiro.
Copyright 2008 by Strawberry Development Group.  All rights reserved.
      For terms of redistribution, see the <a
      href="../legal/license/GPL.html">GNU General Public License</a></em>
This material is based upon work supported by the US Defense Advanced
Research Projects Agency under Contract No. W31P4Q-07-C-0070.
Approved for public release, distribution unlimited.
   </td>
</tr>
</table>
  </body>
</html>
